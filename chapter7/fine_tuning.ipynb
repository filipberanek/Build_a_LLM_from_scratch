{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.10.0\n",
      "tiktoken version: 0.8.0\n",
      "torch version: 2.5.1\n",
      "tqdm version: 4.67.1\n",
      "tensorflow version: 2.18.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import numpy as np\n",
    "\n",
    "pkgs = [\n",
    "    \"matplotlib\",  # Plotting library\n",
    "    \"tiktoken\",    # Tokenizer\n",
    "    \"torch\",       # Deep learning library\n",
    "    \"tqdm\",        # Progress bar\n",
    "    \"tensorflow\",  # For OpenAI's pretrained weights\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "\n",
    "def download_and_load_file(file_path, url):\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "file_path = \"instruction-data.json\"\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\" \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    ")\n",
    "\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example entry:\n",
      " {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n"
     ]
    }
   ],
   "source": [
    "print(\"Example entry:\\n\", data[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion\n",
      "\n",
      "### Response:\n",
      "The correct spelling is 'Occasion.'\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[50])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is an antonym of 'complicated'?\n",
      "\n",
      "### Response:\n",
      "An antonym of 'complicated' is 'simple'.\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[999])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[999]['output']}\"\n",
    "\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Name a synonym for \"happiness.\"',\n",
       " 'input': '',\n",
       " 'output': 'A synonym for \"happiness\" is \"joy.\"'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nIdentify the correct spelling of the following word.\\n\\n### Input:\\nOcassion'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_input(data[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split to train, val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_portion = int(len(data) * 0.85)  # 85% for training\n",
    "test_portion = int(len(data) * 0.1)  # 10% for testing\n",
    "val_portion = len(data) - train_portion - test_portion  # Remaining 5% for validation\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion : train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 935\n",
      "Validation set length: 55\n",
      "Test set length: 110\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing data into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            self.encoded_texts.append(tokenizer.encode(full_text))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a collate function to handle adding extra tokens \"endoftext\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_draft_1(batch, pad_token_id=50256, device=\"cpu\"):\n",
    "    # Find the longest sequence in the batch\n",
    "    # and increase the max length by +1, which will add one extra\n",
    "    # padding token below\n",
    "    batch_max_length = max(len(item) + 1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs\n",
    "    inputs_lst = []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to batch_max_length\n",
    "        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        # Via padded[:-1], we remove the extra padded token\n",
    "        # that has been added via the +1 setting in batch_max_length\n",
    "        # (the extra padding token will be relevant in later codes)\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        #inputs = torch.tensor(padded)\n",
    "        inputs_lst.append(inputs)\n",
    "\n",
    "    # Convert list of inputs to tensor and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    return inputs_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "\n",
    "batch = (inputs_1, inputs_2, inputs_3)\n",
    "\n",
    "print(custom_collate_draft_1(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement targets into collate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_draft_2(batch, pad_token_id=50256, device=\"cpu\"):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item) + 1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to max_length\n",
    "        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
    "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # Convert list of inputs to tensor and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256, 50256, 50256, 50256],\n",
      "        [    8,     9, 50256, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = custom_collate_draft_2(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace other endoftext tokens to be -100, so do not contribute to learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch, pad_token_id=50256, ignore_index=-100, allowed_max_length=None, device=\"cpu\"):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item) + 1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs and targets\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to max_length\n",
    "        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
    "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
    "\n",
    "        # New: Replace all but the first padding tokens in targets by ignore_index\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "\n",
    "        # New: Optionally truncate to maximum sequence length\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # Convert list of inputs and targets to tensors and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256,  -100,  -100,  -100],\n",
      "        [    8,     9, 50256,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = custom_collate_fn(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n"
     ]
    }
   ],
   "source": [
    "logits_1 = torch.tensor([[-1.0, 1.0], [-0.5, 1.5]])  # 1st training example  # 2nd training example\n",
    "targets_1 = torch.tensor([0, 1])\n",
    "\n",
    "\n",
    "loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)\n",
    "print(loss_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7936)\n"
     ]
    }
   ],
   "source": [
    "logits_2 = torch.tensor([[-1.0, 1.0], [-0.5, 1.5], [-0.5, 1.5]])  # New 3rd training example\n",
    "targets_2 = torch.tensor([0, 1, 1])\n",
    "\n",
    "loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\n",
    "print(loss_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n",
      "loss_1 == loss_3: tensor(True)\n"
     ]
    }
   ],
   "source": [
    "targets_3 = torch.tensor([0, 1, -100])\n",
    "\n",
    "loss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)\n",
    "print(loss_3)\n",
    "print(\"loss_1 == loss_3:\", loss_1 == loss_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    As we can see, the resulting loss on these 3 training examples is the same as the loss we calculated from the 2 training examples, which means that the cross-entropy loss function ignored the training example with the -100 label\n",
    "    By default, PyTorch has the cross_entropy(..., ignore_index=-100) setting to ignore examples corresponding to the label -100\n",
    "    Using this -100 ignore_index, we can ignore the additional end-of-text (padding) tokens in the batches that we used to pad the training examples to equal length\n",
    "    However, we don't want to ignore the first instance of the end-of-text (padding) token (50256) because it can help signal to the LLM when the response is complete\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "\n",
    "# Note:\n",
    "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
    "# which is much faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
    "# However, the resulting loss values may be slightly different.\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#    device = torch.device(\"cuda\")\n",
    "# elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\")\n",
    "# else:\n",
    "#    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "customized_collate_fn = partial(custom_collate_fn, device=device, allowed_max_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset.__getitem__(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nIdentify the correct spelling of the following word.\\n\\n### Input:\\nOcassion\\n\\n### Response:\\nThe correct spelling is 'Occasion.'\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_dataset.__getitem__(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for check_input, check_output in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nRewrite the following sentence so that it is in active voice.\\n\\n### Input:\\nThe cake was baked by Sarah.\\n\\n### Response:\\nSarah baked the cake.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(check_input[0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nRewrite the following sentence so that it is in active voice.\\n\\n### Input:\\nThe cake was baked by Sarah.\\n\\n### Response:\\nSarah baked the cake.<|endoftext|>'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(\n",
    "    np.where(check_output[0].detach().cpu().numpy() == -100, 50256, check_output[0].detach().cpu().numpy())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 57]) torch.Size([8, 57])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 88]) torch.Size([8, 88])\n",
      "torch.Size([8, 57]) torch.Size([8, 57])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 58]) torch.Size([8, 58])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 82]) torch.Size([8, 82])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 87]) torch.Size([8, 87])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 89]) torch.Size([8, 89])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
      "          257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n",
      "        21017, 46486,    25,   198, 42779,   597, 24993, 10135,   287,   262,\n",
      "         1813,  6827,    13,   198,   198, 21017, 23412,    25,   198,   464,\n",
      "          307,   315,  4135, 11376,   373,  5901,   351, 12734,    13,   198,\n",
      "          198, 21017, 18261,    25,   198,   464,  4950, 11376,   373,  5901,\n",
      "          351, 12734,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256])\n"
     ]
    }
   ],
   "source": [
    "print(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,   257,\n",
      "         2882,   326, 20431, 32543,   262,  2581,    13,   198,   198, 21017,\n",
      "        46486,    25,   198, 42779,   597, 24993, 10135,   287,   262,  1813,\n",
      "         6827,    13,   198,   198, 21017, 23412,    25,   198,   464,   307,\n",
      "          315,  4135, 11376,   373,  5901,   351, 12734,    13,   198,   198,\n",
      "        21017, 18261,    25,   198,   464,  4950, 11376,   373,  5901,   351,\n",
      "        12734,    13, 50256,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100])\n"
     ]
    }
   ],
   "source": [
    "print(targets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a pretrained LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 13:17:33.137225: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-03 13:17:33.148225: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738585053.160562   12205 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738585053.163812   12205 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-03 13:17:33.177140: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/355M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/355M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/355M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/355M/vocab.bpe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 1024)\n",
       "  (pos_emb): Embedding(1024, 1024)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (12): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (13): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (14): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (15): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (16): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (17): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (18): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (19): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (20): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (21): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (22): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (23): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from previous_chapters import GPTModel, load_weights_into_gpt\n",
    "\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,  # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,  # Dropout rate\n",
    "    \"qkv_bias\": True,  # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "input_text = format_input(val_data[0])\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from previous_chapters import generate, text_to_token_ids, token_ids_to_text\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(input_text, tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    eos_id=50256,\n",
    ")\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
      "\n",
      "### Response:\n",
      "\n",
      "The chef cooks the meal every day.\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "Convert the active sentence to passive: 'The chef cooks the\n"
     ]
    }
   ],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from previous_chapters import calc_loss_loader, train_model_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.8259037017822264\n",
      "Validation loss: 3.7619290351867676\n"
     ]
    }
   ],
   "source": [
    "model.to(device=device)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(data_loader=train_loader, model=model, device=device,num_batches=5)\n",
    "    val_loss = calc_loss_loader(data_loader=val_loader, model=model, device=device, num_batches=5)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 2.637, Val loss 2.626\n",
      "Ep 1 (Step 000005): Train loss 1.174, Val loss 1.102\n",
      "Ep 1 (Step 000010): Train loss 0.872, Val loss 0.945\n",
      "Ep 1 (Step 000015): Train loss 0.857, Val loss 0.906\n",
      "Ep 1 (Step 000020): Train loss 0.776, Val loss 0.881\n",
      "Ep 1 (Step 000025): Train loss 0.754, Val loss 0.859\n",
      "Ep 1 (Step 000030): Train loss 0.799, Val loss 0.836\n",
      "Ep 1 (Step 000035): Train loss 0.714, Val loss 0.808\n",
      "Ep 1 (Step 000040): Train loss 0.672, Val loss 0.806\n",
      "Ep 1 (Step 000045): Train loss 0.633, Val loss 0.790\n",
      "Ep 1 (Step 000050): Train loss 0.662, Val loss 0.783\n",
      "Ep 1 (Step 000055): Train loss 0.760, Val loss 0.764\n",
      "Ep 1 (Step 000060): Train loss 0.719, Val loss 0.743\n",
      "Ep 1 (Step 000065): Train loss 0.652, Val loss 0.735\n",
      "Ep 1 (Step 000070): Train loss 0.532, Val loss 0.729\n",
      "Ep 1 (Step 000075): Train loss 0.569, Val loss 0.729\n",
      "Ep 1 (Step 000080): Train loss 0.605, Val loss 0.725\n",
      "Ep 1 (Step 000085): Train loss 0.509, Val loss 0.709\n",
      "Ep 1 (Step 000090): Train loss 0.562, Val loss 0.691\n",
      "Ep 1 (Step 000095): Train loss 0.500, Val loss 0.681\n",
      "Ep 1 (Step 000100): Train loss 0.502, Val loss 0.676\n",
      "Ep 1 (Step 000105): Train loss 0.564, Val loss 0.670\n",
      "Ep 1 (Step 000110): Train loss 0.555, Val loss 0.667\n",
      "Ep 1 (Step 000115): Train loss 0.508, Val loss 0.664\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is prepared every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive:\n",
      "Ep 2 (Step 000120): Train loss 0.435, Val loss 0.672\n",
      "Ep 2 (Step 000125): Train loss 0.451, Val loss 0.686\n",
      "Ep 2 (Step 000130): Train loss 0.447, Val loss 0.683\n",
      "Ep 2 (Step 000135): Train loss 0.405, Val loss 0.682\n",
      "Ep 2 (Step 000140): Train loss 0.410, Val loss 0.681\n",
      "Ep 2 (Step 000145): Train loss 0.369, Val loss 0.682\n",
      "Ep 2 (Step 000150): Train loss 0.381, Val loss 0.676\n",
      "Ep 2 (Step 000155): Train loss 0.412, Val loss 0.676\n",
      "Ep 2 (Step 000160): Train loss 0.414, Val loss 0.684\n",
      "Ep 2 (Step 000165): Train loss 0.379, Val loss 0.687\n",
      "Ep 2 (Step 000170): Train loss 0.323, Val loss 0.682\n",
      "Ep 2 (Step 000175): Train loss 0.337, Val loss 0.671\n",
      "Ep 2 (Step 000180): Train loss 0.392, Val loss 0.658\n",
      "Ep 2 (Step 000185): Train loss 0.416, Val loss 0.659\n",
      "Ep 2 (Step 000190): Train loss 0.340, Val loss 0.650\n",
      "Ep 2 (Step 000195): Train loss 0.329, Val loss 0.637\n",
      "Ep 2 (Step 000200): Train loss 0.310, Val loss 0.637\n",
      "Ep 2 (Step 000205): Train loss 0.352, Val loss 0.633\n",
      "Ep 2 (Step 000210): Train loss 0.367, Val loss 0.631\n",
      "Ep 2 (Step 000215): Train loss 0.396, Val loss 0.635\n",
      "Ep 2 (Step 000220): Train loss 0.301, Val loss 0.648\n",
      "Ep 2 (Step 000225): Train loss 0.348, Val loss 0.661\n",
      "Ep 2 (Step 000230): Train loss 0.295, Val loss 0.657\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is cooked every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: What is the capital of the United Kingdom\n",
      "Training completed in 21.25 minutes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "torch.manual_seed(123)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=0.00005, weight_decay=0.1\n",
    ")\n",
    "num_epochs = 2\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(model, train_loader, val_loader, \n",
    "                                                           optimizer, device, num_epochs, eval_freq=5, \n",
    "                                                           eval_iter=5, \n",
    "                                                           start_context=format_input(val_data[0]), \n",
    "                                                           tokenizer=tokenizer)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from previous_chapters import plot_losses\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The car is very fast.\n",
      "\n",
      "Correct response: \n",
      ">>The car is as fast as lightning.\n",
      "\n",
      "Model response: \n",
      ">>The car is as fast as a bullet.\n",
      "-------------------------------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What type of cloud is typically associated with thunderstorms?\n",
      "\n",
      "Correct response: \n",
      ">>The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response: \n",
      ">>The type of cloud associated with thunderstorms is a cumulus cloud.\n",
      "-------------------------------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Name the author of 'Pride and Prejudice'.\n",
      "\n",
      "Correct response: \n",
      ">>Jane Austen.\n",
      "\n",
      "Model response: \n",
      ">>The author of 'Pride and Prejudice' is Jane Austen.\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from previous_chapters import generate\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "for entry in test_data[:3]:\n",
    "    input_text = format_input(entry)\n",
    "    token_ids = generate(model = model, \n",
    "                         idx = text_to_token_ids(input_text, tokenizer).to(device),\n",
    "                         max_new_tokens = 256, \n",
    "                         context_size = BASE_CONFIG[\"context_length\"], \n",
    "                         eos_id = 50256)\n",
    "    generated_text = token_ids_to_text(token_ids=token_ids, tokenizer=tokenizer)\n",
    "    resonse_text = (generated_text[len(input_text):].replace(\"### Response:\", \"\").strip())\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response: \\n>>{entry['output']}\")\n",
    "    print(f\"\\nModel response: \\n>>{resonse_text.strip()}\")\n",
    "    print(\"-------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save responses to the test set and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 110/110 [08:38<00:00,  4.72s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "    input_text = format_input(entry)\n",
    "    # Convert to token ids\n",
    "    token_ids = generate(model=model, \n",
    "                         idx=text_to_token_ids(input_text, tokenizer=tokenizer).to(device), \n",
    "                         max_new_tokens=256, \n",
    "                         context_size=BASE_CONFIG[\"context_length\"], \n",
    "                         eos_id=50256)\n",
    "    generated_text = token_ids_to_text(token_ids=token_ids, tokenizer=tokenizer)\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):].replace(\"### Response:\",\"\").strip()\n",
    "    )\n",
    "    test_data[i][\"model_response\"] = response_text\n",
    "\n",
    "with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
    "    json.dump(test_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'What is the capital of Denmark?',\n",
       " 'input': '',\n",
       " 'output': 'The capital of Denmark is Copenhagen.',\n",
       " 'model_response': 'The capital of Denmark is Copenhagen.'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt2-medium355M-sft.pth'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = f\"{re.sub(r'[ ()]','',CHOOSE_MODEL)}-sft.pth\"\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been saved\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), filename)\n",
    "print(\"Model has been saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation the fine-tuned LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use ollama phi3 for evaluation of out finetuned model\n",
    "</br>\n",
    "To run ollama or phi model, run: ollama run phi3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "def check_if_running(process_name):\n",
    "    running = False\n",
    "    for proc in psutil.process_iter([\"name\"]):\n",
    "        if process_name in proc.info[\"name\"]:\n",
    "            running = True\n",
    "            break\n",
    "    return running\n",
    "\n",
    "ollama_running = check_if_running(\"ollama\")\n",
    "\n",
    "if not ollama_running:\n",
    "    raise RuntimeError(\"Ollama is not running. Lunch ollama before proceeding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (3844165512.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[54], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    url=\"http://localhost:11434/api/chat\")\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "def query_model(prompt, \n",
    "                model = \"phi3\", # Its smaller for 8GB, otherwise use llama3\n",
    "                url=\"http://localhost:11434/api/chat\"):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
