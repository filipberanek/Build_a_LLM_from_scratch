{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check libraries and its versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.10.0\n",
      "numpy version: 2.0.2\n",
      "tiktoken version: 0.8.0\n",
      "torch version: 2.5.1\n",
      "tensorflow version: 2.18.0\n",
      "pandas version: 2.2.3\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\",\n",
    "        \"numpy\",\n",
    "        \"tiktoken\",\n",
    "        \"torch\",\n",
    "        \"tensorflow\", # For OpenAI's pretrained weights\n",
    "        \"pandas\"      # Dataset loading\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get training dataset for spam classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sms_spam_collection/SMSSpamCollection.tsv already exists. Skipping download and extraction.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from lora_previous_chapters import (\n",
    "    download_and_unzip_spam_data,\n",
    "    create_balanced_dataset,\n",
    "    random_split\n",
    ")\n",
    "\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
    "\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Dude how do you like the buff wind.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Tessy..pls do me a favor. Pls convey my birthd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Reminder: You have not downloaded the content ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Got what it takes 2 take part in the WRC Rally...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Shop till u Drop, IS IT YOU, either 10K, 5K, £...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1040</th>\n",
       "      <td>1</td>\n",
       "      <td>4mths half price Orange line rental &amp; latest c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>1</td>\n",
       "      <td>Thanks for the Vote. Now sing along with the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>1</td>\n",
       "      <td>IMPORTANT INFORMATION 4 ORANGE USER 0796XXXXXX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>1</td>\n",
       "      <td>Urgent! call 09066612661 from landline. Your c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>0</td>\n",
       "      <td>His frens go then he in lor. Not alone wif my ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1045 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Label                                               Text\n",
       "0         0                Dude how do you like the buff wind.\n",
       "1         0  Tessy..pls do me a favor. Pls convey my birthd...\n",
       "2         1  Reminder: You have not downloaded the content ...\n",
       "3         1  Got what it takes 2 take part in the WRC Rally...\n",
       "4         1  Shop till u Drop, IS IT YOU, either 10K, 5K, £...\n",
       "...     ...                                                ...\n",
       "1040      1  4mths half price Orange line rental & latest c...\n",
       "1041      1  Thanks for the Vote. Now sing along with the s...\n",
       "1042      1  IMPORTANT INFORMATION 4 ORANGE USER 0796XXXXXX...\n",
       "1043      1  Urgent! call 09066612661 from landline. Your c...\n",
       "1044      0  His frens go then he in lor. Not alone wif my ...\n",
       "\n",
       "[1045 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import tiktoken\n",
    "from lora_previous_chapters import SpamDataset\n",
    "\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "train_dataset = SpamDataset(\"train.csv\", max_length=None, tokenizer=tokenizer)\n",
    "val_dataset = SpamDataset(\"validation.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)\n",
    "test_dataset = SpamDataset(\"test.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   35,  2507,   703,   466,   345,   588,   262,  6940,  2344,    13,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dude how do you like the buff wind.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_dataset.__getitem__(0)[0].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check dataloaders, that they contain 8 batches with 120 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "Input batch dimensions: torch.Size([8, 120])\n",
      "Label batch dimensions torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print number of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 training batches\n",
      "19 validation batches\n",
      "38 test batches\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 15:42:40.858148: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-04 15:42:40.867763: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738680160.879637   20496 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738680160.883274   20496 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-04 15:42:40.896873: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lora_gpt_download import download_and_load_gpt2\n",
    "from lora_previous_chapters import GPTModel, load_weights_into_gpt\n",
    "\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,  # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,  # Dropout rate\n",
    "    \"qkv_bias\": True,  # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check model, that weights were loaded properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you forward.\n",
      "\n",
      "The first step is to understand the importance of your work\n"
     ]
    }
   ],
   "source": [
    "from lora_previous_chapters import generate_text_simple, text_to_token_ids, token_ids_to_text\n",
    "\n",
    "\n",
    "text_1 = \"Every effort moves you\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We change conversational tool into classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(in_features=768, out_features=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Note:\n",
    "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
    "# which is approximately 1.2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
    "# However, the resulting loss values may be slightly different.\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#    device = torch.device(\"cuda\")\n",
    "# elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\")\n",
    "# else:\n",
    "#    device = torch.device(\"cpu\")\n",
    "#\n",
    "# print(f\"Using {device} device.\")\n",
    "\n",
    "model.to(device)\n",
    "# no assignment model = model.to(device) necessary for nn.Module classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "from lora_previous_chapters import calc_accuracy_loader\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement LORA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we implement LORA as two parameters that multiplies each other and then apply scaling factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class LoraLayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))\n",
    "        torch.nn.init.kaiming_uniform(self.A, a = math.sqrt(5))\n",
    "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.alpha * (x @ self.A @ self.B)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To integrate the original \"Linear\" layer weights, we now create a LinearWithLora layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWithLora(torch.nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoraLayer(linear.in_features, linear.out_features, rank, alpha)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement our new linear implementation into original weight, we create function \"replace_linear_with_lora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear_with_lora(model, rank, alpha):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            setattr(model, name, LinearWithLora(module, rank, alpha))\n",
    "        else:\n",
    "            replace_linear_with_lora(module, rank, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We freeze weights before applying lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters before: 124,441,346\n",
      "Total trainable parameters after: 0\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters before: {total_params:,}\")\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters after: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable LoRA parameters: 2,666,528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20496/2602388664.py:7: FutureWarning: `nn.init.kaiming_uniform` is now deprecated in favor of `nn.init.kaiming_uniform_`.\n",
      "  torch.nn.init.kaiming_uniform(self.A, a = math.sqrt(5))\n"
     ]
    }
   ],
   "source": [
    "replace_linear_with_lora(model, rank=16, alpha=16)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable LoRA parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets verify, that layer were updated to LORA style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (W_key): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (W_value): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLora(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoraLayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLora(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoraLayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (W_key): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (W_value): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLora(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoraLayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLora(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoraLayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (W_key): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (W_value): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLora(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoraLayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLora(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoraLayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (W_key): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (W_value): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLora(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoraLayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLora(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoraLayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (W_key): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (W_value): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLora(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoraLayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLora(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoraLayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (W_key): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (W_value): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLora(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoraLayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLora(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoraLayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (W_key): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (W_value): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLora(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoraLayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLora(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoraLayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (W_key): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (W_value): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLora(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoraLayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLora(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoraLayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (W_key): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (W_value): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLora(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoraLayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLora(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoraLayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (W_key): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (W_value): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLora(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoraLayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLora(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoraLayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (W_key): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (W_value): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLora(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoraLayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLora(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoraLayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (W_key): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (W_value): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLora(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoraLayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLora(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoraLayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLora(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoraLayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): LinearWithLora(\n",
      "    (linear): Linear(in_features=768, out_features=2, bias=True)\n",
      "    (lora): LoraLayer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 0.527, Val loss 0.427\n",
      "Ep 1 (Step 000050): Train loss 0.164, Val loss 0.181\n",
      "Ep 1 (Step 000100): Train loss 0.180, Val loss 0.481\n",
      "Training accuracy: 95.00% | Validation accuracy: 97.50%\n",
      "Ep 2 (Step 000150): Train loss 0.197, Val loss 0.098\n",
      "Ep 2 (Step 000200): Train loss 0.033, Val loss 0.020\n",
      "Ep 2 (Step 000250): Train loss 0.060, Val loss 0.047\n",
      "Training accuracy: 97.50% | Validation accuracy: 95.00%\n",
      "Ep 3 (Step 000300): Train loss 0.046, Val loss 0.072\n",
      "Ep 3 (Step 000350): Train loss 0.067, Val loss 0.006\n",
      "Training accuracy: 100.00% | Validation accuracy: 95.00%\n",
      "Ep 4 (Step 000400): Train loss 0.005, Val loss 0.036\n",
      "Ep 4 (Step 000450): Train loss 0.054, Val loss 0.107\n",
      "Ep 4 (Step 000500): Train loss 0.044, Val loss 0.398\n",
      "Training accuracy: 100.00% | Validation accuracy: 95.00%\n",
      "Training completed in 1.02 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from lora_previous_chapters import train_classifier_simple\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs=num_epochs,\n",
    "    eval_freq=50,\n",
    "    eval_iter=5,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lora_previous_chapters import plot_values\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses, label=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 98.46%\n",
      "Validation accuracy: 95.97%\n",
      "Test accuracy: 96.33%\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
